---
title: "Support Matrix"
description: "Supported LLM providers, agent frameworks, and features"
---

## LLM Providers

| Provider | Status | Instrumentation Method | Features |
|----------|--------|----------------------|----------|
| OpenAI | âœ… Supported | `instrument()` | Streaming, Tool calls, Vision, JSON mode |
| Anthropic | âœ… Supported | `instrument()` | Streaming, Tool calls, Prompt caching |
| Google Gemini | âœ… Supported | `instrument()` | Streaming, Tool calls, Chat sessions |

## Agent Frameworks

| Framework | Status | Instrumentation Method | Notes |
|-----------|--------|----------------------|-------|
| [Vercel AI SDK](https://sdk.vercel.ai) | âœ… Supported | `instrumentFetch()` | generateText, streamText, generateObject |
| [LangChain.js](https://js.langchain.com) | âœ… Supported | `instrumentFetch()` | Chains, LCEL, agents, tool binding |
| [LlamaIndex.ts](https://ts.llamaindex.ai) | âœ… Supported | `instrumentFetch()` | RAG pipelines, chat engines |
| [Mastra](https://mastra.ai) | âœ… Supported | `instrumentFetch()` + `withAgent()` | Agents, tools, workflows |

<Note>
Frameworks that bundle their own SDK copies require `instrumentFetch()` to intercept HTTP calls. Direct SDK usage can use `instrument()`.
</Note>

## Feature Support

| Feature | OpenAI | Anthropic | Gemini |
|---------|--------|-----------|--------|
| Basic completions | âœ… | âœ… | âœ… |
| Streaming | âœ… | âœ… | âœ… |
| Tool/Function calls | âœ… | âœ… | âœ… |
| Token tracking | âœ… | âœ… | âœ… |
| Cached token tracking | âœ… | âœ… | âŒ |
| Rate limit tracking | âœ… | âœ… | âŒ |
| Latency metrics | âœ… | âœ… | âœ… |
| Error tracking | âœ… | âœ… | âœ… |
| Multi-agent tracking | âœ… | âœ… | âœ… |
| Cost control | âœ… | âœ… | âœ… |

## Instrumentation Methods

### `instrument()` - SDK Instrumentation

Best for direct SDK usage. Patches SDK prototypes at startup.

```typescript
import { instrument } from "aden";
import OpenAI from "openai";
import Anthropic from "@anthropic-ai/sdk";
import { GoogleGenerativeAI } from "@google/generative-ai";

await instrument({
  emitMetric: myEmitter,
  sdks: { OpenAI, Anthropic, GoogleGenerativeAI },
});
```

**Use when:**
- Using OpenAI, Anthropic, or Gemini SDKs directly
- Building custom agents without a framework

### `instrumentFetch()` - HTTP Instrumentation

Best for frameworks that make direct HTTP calls or bundle their own SDK copies.

```typescript
import { instrumentFetch } from "aden";

await instrumentFetch({
  emitMetric: myEmitter,
});
```

**Use when:**
- Using Vercel AI SDK
- Using LangChain.js
- Using LlamaIndex.ts
- Using Mastra or other fetch-based frameworks

## Examples

| Example | Description | File |
|---------|-------------|------|
| OpenAI Basic | Completions, streaming, tool calls | `openai-basic.ts` |
| Anthropic Basic | Messages, streaming, prompt caching | `anthropic-basic.ts` |
| Gemini Basic | Content generation, chat sessions | `gemini-basic.ts` |
| Vercel AI SDK | generateText, streamText, generateObject | `vercel-ai-sdk.ts` |
| LangChain | LCEL chains, multi-model, tool binding | `langchain-example.ts` |
| LlamaIndex | RAG pipelines, chat interface | `llamaindex-example.ts` |
| Mastra | Agents, tools, multi-agent workflows | `mastra-example.ts` |
| Multi-Agent | Sequential, parallel, debate patterns | `multi-agent-example.ts` |
| Cost Control | Local policy engine without server | `cost-control-local.ts` |
| Control Actions | All 5 control actions demo | `control-actions.ts` |

<Card title="View Examples on GitHub" icon="github" href="https://github.com/adenhq/aden-ts/tree/main/examples">
  Browse complete example code
</Card>

## Coming Soon

| Framework | Status | ETA |
|-----------|--------|-----|
| CrewAI.js | ğŸ”œ Planned | - |
| AutoGen.js | ğŸ”œ Planned | - |

<Info>
Want support for another framework? [Open an issue](https://github.com/adenhq/aden-ts/issues) on GitHub.
</Info>
